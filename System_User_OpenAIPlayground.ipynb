{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg6sPtrb2+dyYxYLnup2tN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlie-aashutosh/Generative_AI/blob/main/System_User_OpenAIPlayground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaoi42F1kZFK",
        "outputId": "be13a35c-f2ea-469c-bc6d-bd7a00cfd56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Set up your system and user messages for GPT interaction:\n",
            "Enter the system message: You are a helpful bot\n",
            "Enter the user message: Explain RAG to me as if I am a 10 year old kid\n",
            "\n",
            "ChatGPT Response:\n",
            "Sure, kiddo! Imagine you're playing with a bunch of different colored Lego blocks. We can compare those blocks to pieces of information or knowledge. Sometimes, when you're building something, you want to select the right color block that fits perfectly into your creation, right?\n",
            "\n",
            "Well, RAG, which stands for Retrieval-Augmented Generation, is kind of like that. It's a method used by computers, like me, to help you with questions or tasks. The \"retrieval\" part is like choosing the right Lego blocks - or the right pieces of information. The \"augmentation\" part is about fitting those selected blocks into the right place to create the final outcome. \n",
            "\n",
            "So, just like you make an awesome Lego castle by using different colored blocks and fitting them right, RAG helps a computer build a good response or solution by using the right pieces of information.\n"
          ]
        }
      ],
      "source": [
        "# Install OpenAI library (uncomment if not already installed)\n",
        "!pip install openai==0.28\n",
        "\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"Your API Key\"  # Replace with your OpenAI API key\n",
        "\n",
        "def chat_with_gpt(messages, model=\"gpt-4\"):\n",
        "    \"\"\"\n",
        "    Interact with the OpenAI GPT model.\n",
        "\n",
        "    Args:\n",
        "        messages (list): A list of messages in the format:\n",
        "            [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, ...]\n",
        "        model (str): The model to use. Default is \"gpt-4\".\n",
        "\n",
        "    Returns:\n",
        "        str: The response from the model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Input system and user messages\n",
        "print(\"Set up your system and user messages for GPT interaction:\")\n",
        "\n",
        "# Input the system message\n",
        "system_message = input(\"Enter the system message: \")\n",
        "\n",
        "# Input the user message\n",
        "user_message = input(\"Enter the user message: \")\n",
        "\n",
        "# Create the message list\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "]\n",
        "\n",
        "# Get the GPT response\n",
        "response = chat_with_gpt(messages)\n",
        "\n",
        "# Display the response\n",
        "print(\"\\nChatGPT Response:\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OpenAI library (uncomment if not already installed)\n",
        "!pip install openai==0.28\n",
        "\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"Your API Key\"  # Replace with your OpenAI API key\n",
        "\n",
        "def chat_with_gpt(messages, model=\"gpt-4\", temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
        "    \"\"\"\n",
        "    Interact with the OpenAI GPT model with adjustable parameters.\n",
        "\n",
        "    Args:\n",
        "        messages (list): A list of messages in the format:\n",
        "            [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, ...]\n",
        "        model (str): The model to use. Default is \"gpt-4\".\n",
        "        temperature (float): Controls randomness. Lower is deterministic; higher is creative. Default is 0.7.\n",
        "        max_tokens (int): Maximum number of tokens to generate. Default is 150.\n",
        "        top_p (float): Controls diversity via nucleus sampling. 1.0 means no sampling limit. Default is 1.0.\n",
        "        frequency_penalty (float): Penalizes repeated phrases. Default is 0.0.\n",
        "        presence_penalty (float): Penalizes repeated topics. Default is 0.0.\n",
        "\n",
        "    Returns:\n",
        "        str: The response from the model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=top_p,\n",
        "            frequency_penalty=frequency_penalty,\n",
        "            presence_penalty=presence_penalty\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Input system and user messages\n",
        "print(\"Set up your system and user messages for GPT interaction:\")\n",
        "\n",
        "# Input the system message\n",
        "system_message = input(\"Enter the system message: \")\n",
        "\n",
        "# Input the user message\n",
        "user_message = input(\"Enter the user message: \")\n",
        "\n",
        "# Adjustable parameters\n",
        "temperature = float(input(\"Enter temperature (0.0 to 1.0, default 0.7): \") or 0.7)\n",
        "max_tokens = int(input(\"Enter max tokens (default 150): \") or 150)\n",
        "top_p = float(input(\"Enter top_p (0.0 to 1.0, default 1.0): \") or 1.0)\n",
        "frequency_penalty = float(input(\"Enter frequency penalty (-2.0 to 2.0, default 0.0): \") or 0.0)\n",
        "presence_penalty = float(input(\"Enter presence penalty (-2.0 to 2.0, default 0.0): \") or 0.0)\n",
        "\n",
        "# Create the message list\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "]\n",
        "\n",
        "# Get the GPT response\n",
        "response = chat_with_gpt(\n",
        "    messages,\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    top_p=top_p,\n",
        "    frequency_penalty=frequency_penalty,\n",
        "    presence_penalty=presence_penalty\n",
        ")\n",
        "\n",
        "# Display the response\n",
        "print(\"\\nChatGPT Response:\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y99H4yoPkjFV",
        "outputId": "e19c4df1-4486-46f5-8128-0fa23b913fae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Set up your system and user messages for GPT interaction:\n",
            "Enter the system message: You are a sarcastic bot\n",
            "Enter the user message: Give a joke to make me laugh \n",
            "Enter temperature (0.0 to 1.0, default 0.7): 0.7\n",
            "Enter max tokens (default 150): 100\n",
            "Enter top_p (0.0 to 1.0, default 1.0): 1\n",
            "Enter frequency penalty (-2.0 to 2.0, default 0.0): 0\n",
            "Enter presence penalty (-2.0 to 2.0, default 0.0): 0\n",
            "\n",
            "ChatGPT Response:\n",
            "Oh, sure, because I'm just a hilarious stand-up comedian, right? Here goes, \"Why don't scientists trust atoms?\" Because they make up everything! I'm here all week folks, don't forget to tip your servers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UbvdeGd0l-Ti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}